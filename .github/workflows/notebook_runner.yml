name: Single-cell notebooks runner
run-name: Single-cell notebooks runner --- triggered by ${{ github.actor }}

on:
  pull_request: {}
  push:
    branches: [main, master, staging, release/*]
  workflow_dispatch:
    inputs:
      environment:
        description: "Deploy Environment"
        required: true
        default: "staging"
      debug_from_step:
        description: 'Start from step (e.g. "load-docker")'
        required: false
        default: ""
      target_branch:
        description: "Branch to run on (e.g. main)"
        required: false
        default: ""

permissions:
  checks: write
  contents: read

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  run-notebook:
    runs-on: arc-runner-set-oke-org-poc-4-gpu
    environment: dev
    strategy:
      matrix:
        NOTEBOOK_FILENAME: [
            "01_scRNA_analysis_preprocessing.ipynb",
            # "02_scRNA_analysis_extended.ipynb",  # Temporarily disabled due to GPU resource constraints
            "03_scRNA_analysis_with_pearson_residuals.ipynb",
            "04_scRNA_analysis_dask_out_of_core.ipynb",
            "05_scRNA_analysis_multi_GPU.ipynb",
            "06_scRNA_analysis_90k_brain_example.ipynb",
            "07_scRNA_analysis_1.3M_brain_example.ipynb"
          ]
      max-parallel: 1  # Reduced from 4 to avoid GPU resource conflicts and UCX/CUDA context issues
      fail-fast: false

    env:
      DEPLOY_ENV: ${{ github.event_name == 'workflow_dispatch' && inputs.environment || 'dev' }}

      #### VARS
      DOCKER_IMG_NAME: "nvcr.io/nvidia/rapidsai/notebooks"
      DOCKER_IMG_TAG: "25.06-cuda12.8-py3.12"
      DOCKER_COMPOSE_FILE: "${{ github.workspace }}/docker/brev/docker-compose-nb-2504.yaml"
      NOTEBOOK_RELATIVED_DIR: "notebooks"
      PYTHON_VERSION: "3.12"

      ### CONSTANT
      DOCKER_WRITEABLE_DIR: "/notebooks"
      DOCKER_CACHE_DIR: "docker-pull-cache"
      HOST_CACHE_DIR: "${{ github.workspace }}/docker-pull-cache"
      OUTPUT_NOTEBOOK: "result_${{ matrix.NOTEBOOK_FILENAME }}"
      OUTPUT_NOTEBOOK_HTML: "result_${{ matrix.NOTEBOOK_FILENAME }}.html"
      OUTPUT_PYTEST_COVERAGE_XML: "pytest_coverage_${{ matrix.NOTEBOOK_FILENAME }}.xml"
      OUTPUT_PYTEST_RESULT_XML: "pytest_result_${{ matrix.NOTEBOOK_FILENAME }}.xml"
      OUTPUT_PYTEST_REPORT_HTML: "pytest_report_${{ matrix.NOTEBOOK_FILENAME }}.html"
      ARTIFACT_DIR: "${{ github.workspace }}/test-results/${{ matrix.NOTEBOOK_FILENAME }}"

    steps:
      - name: Set global vars
        id: set_global_vars
        run: |
          DOCKER_IMG_CACHE_NAME=$(echo "$DOCKER_IMG_NAME:$DOCKER_IMG_TAG" | sed 's/[\/:@.]/_/g')
          echo "Cache filename: ${DOCKER_IMG_CACHE_NAME}.tar"
          echo "docker_img_cache_name=${DOCKER_IMG_CACHE_NAME}" >> $GITHUB_OUTPUT

          NOTEBOOK_FILENAME="${{ matrix.NOTEBOOK_FILENAME }}"
          NOTEBOOK_BASENAME=$(basename "$NOTEBOOK_FILENAME" | cut -d. -f1)
          echo "notebook_filename=${NOTEBOOK_FILENAME}" >> $GITHUB_OUTPUT
          echo "notebook_basename=${NOTEBOOK_BASENAME}" >> $GITHUB_OUTPUT

          echo "cache_key=${{ runner.os }}-docker-$DOCKER_IMG_CACHE_NAME" >> $GITHUB_OUTPUT

          ARTIFACT_CLEAN_NAME=$(echo "$NOTEBOOK_BASENAME" | tr -d '"<>:|*?\\/')
          echo "artifact_name=results-$ARTIFACT_CLEAN_NAME" >> $GITHUB_OUTPUT

      - name: Checkout BP repository
        uses: actions/checkout@v4

      # ============================================================
      # PRE-INSTALL: Install all required software and tools
      # ============================================================
      - name: Pre-Install - Install all required software and tools
        id: pre-install
        run: |
          echo "=========================================="
          echo "ðŸ”§ PRE-INSTALL: Installing all required software and tools"
          echo "=========================================="
          
          # Update package lists
          sudo apt-get update
          
          # Install basic utilities
          echo "ðŸ“¦ Installing basic utilities..."
          sudo apt-get install -y wget curl git build-essential jq libxml2-utils ca-certificates gnupg
          
          # Install Docker and Docker Compose
          echo "ðŸ³ Installing Docker and Docker Compose..."
          if ! command -v docker &> /dev/null; then
            curl -fsSL https://get.docker.com -o get-docker.sh
            sudo sh get-docker.sh
            rm -f get-docker.sh
          else
            echo "Docker already installed, skipping..."
          fi
          
          # Add Docker's official GPG key and repository for docker-compose-plugin
          if ! docker compose version &> /dev/null; then
            echo "Installing docker-compose-plugin from Docker official repository..."
            sudo install -m 0755 -d /etc/apt/keyrings
            curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg --yes
            sudo chmod a+r /etc/apt/keyrings/docker.gpg
            echo \
              "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
              $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
              sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
            sudo apt-get update
            sudo apt-get install -y docker-compose-plugin
          else
            echo "Docker Compose plugin already installed, skipping..."
          fi
          
          # Install standalone docker-compose as fallback
          if ! command -v docker-compose &> /dev/null; then
            echo "Installing standalone docker-compose..."
            sudo apt-get install -y docker-compose || true
          fi
          
          # Ensure docker service is running
          sudo systemctl start docker || true
          sudo usermod -aG docker $USER || true
          
          echo ""
          echo "âœ… Verifying installations..."
          echo "-------------------------------------------"
          echo "Docker:         $(docker --version)"
          echo "Docker Compose: $(docker compose version)"
          echo "Git:            $(git --version)"
          echo "Curl:           $(curl --version | head -1)"
          echo "Wget:           $(wget --version | head -1)"
          echo "Jq:             $(jq --version)"
          echo "-------------------------------------------"
          
          echo ""
          echo "=========================================="
          echo "âœ… PRE-INSTALL: All software installed successfully"
          echo "=========================================="

      # ============================================================
      # PRE-CHECK: Validate all prerequisites before execution
      # ============================================================
      - name: Pre-Check - Validate all prerequisites
        id: pre-check
        run: |
          echo "=========================================="
          echo "ðŸ” PRE-CHECK: Validating all prerequisites"
          echo "=========================================="
          
          ERRORS=0
          WARNINGS=0
          
          # ===== Check installed software =====
          echo ""
          echo "ðŸ“‹ [1/5] Checking installed software..."
          echo "-------------------------------------------"
          
          for cmd in docker git curl wget jq; do
            if command -v $cmd &> /dev/null; then
              echo "  âœ… $cmd is available"
            else
              echo "  âŒ $cmd is NOT available"
              ERRORS=$((ERRORS + 1))
            fi
          done
          
          # Check Docker Compose
          if docker compose version &> /dev/null; then
            echo "  âœ… docker compose is available"
          else
            echo "  âŒ docker compose is NOT available"
            ERRORS=$((ERRORS + 1))
          fi
          
          # Check Docker daemon is running
          if docker info &> /dev/null; then
            echo "  âœ… Docker daemon is running"
          else
            echo "  âŒ Docker daemon is NOT running"
            ERRORS=$((ERRORS + 1))
          fi
          
          # ===== Check secrets/tokens =====
          echo ""
          echo "ðŸ”‘ [2/5] Checking secrets and tokens..."
          echo "-------------------------------------------"
          
          # Check BLUEPRINT_GITHUB_TEST_TOKEN_ON_GH
          if [ -n "${{ secrets.BLUEPRINT_GITHUB_TEST_TOKEN_ON_GH }}" ]; then
            echo "  âœ… BLUEPRINT_GITHUB_TEST_TOKEN_ON_GH is set"
            
            # Validate token access to repository
            RESPONSE=$(curl -s -o /dev/null -w "%{http_code}" \
              -H "Authorization: Bearer ${{ secrets.BLUEPRINT_GITHUB_TEST_TOKEN_ON_GH }}" \
              -H "Accept: application/vnd.github.v3+json" \
              "https://api.github.com/repos/NVIDIA-AI-Blueprints/blueprint-github-test")
            
            if [ "$RESPONSE" = "200" ]; then
              echo "  âœ… Token has valid access to NVIDIA-AI-Blueprints/blueprint-github-test"
            elif [ "$RESPONSE" = "404" ]; then
              echo "  âŒ Repository not found or token lacks access (HTTP 404)"
              ERRORS=$((ERRORS + 1))
            elif [ "$RESPONSE" = "401" ]; then
              echo "  âŒ Token is invalid or expired (HTTP 401)"
              ERRORS=$((ERRORS + 1))
            elif [ "$RESPONSE" = "403" ]; then
              echo "  âŒ Token lacks required permissions (HTTP 403)"
              ERRORS=$((ERRORS + 1))
            else
              echo "  âš ï¸ Unexpected response: HTTP $RESPONSE"
              WARNINGS=$((WARNINGS + 1))
            fi
          else
            echo "  âŒ BLUEPRINT_GITHUB_TEST_TOKEN_ON_GH is NOT set"
            ERRORS=$((ERRORS + 1))
          fi
          
          # Check NGC_API_KEY
          if [ -n "${{ secrets.NGC_API_KEY }}" ]; then
            echo "  âœ… NGC_API_KEY is set"
          else
            echo "  âš ï¸ NGC_API_KEY is NOT set (may be required for private NGC images)"
            WARNINGS=$((WARNINGS + 1))
          fi
          
          # Check GITHUB_TOKEN
          if [ -n "${{ secrets.GITHUB_TOKEN }}" ]; then
            echo "  âœ… GITHUB_TOKEN is available"
          else
            echo "  âš ï¸ GITHUB_TOKEN is NOT available"
            WARNINGS=$((WARNINGS + 1))
          fi
          
          # ===== Check external URLs accessibility =====
          echo ""
          echo "ðŸŒ [3/5] Checking external URLs accessibility..."
          echo "-------------------------------------------"
          
          # GitHub API
          HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" --connect-timeout 10 "https://api.github.com")
          if [[ "$HTTP_CODE" =~ ^2 ]]; then
            echo "  âœ… GitHub API (api.github.com) is accessible"
          else
            echo "  âŒ GitHub API (api.github.com) is NOT accessible (HTTP $HTTP_CODE)"
            ERRORS=$((ERRORS + 1))
          fi
          
          # Docker registry (nvcr.io)
          DOCKER_REGISTRY="nvcr.io"
          HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" --connect-timeout 10 "https://${DOCKER_REGISTRY}/v2/")
          if [[ "$HTTP_CODE" =~ ^(2|4) ]]; then
            echo "  âœ… Docker registry (${DOCKER_REGISTRY}) is accessible"
          else
            echo "  âš ï¸ Docker registry (${DOCKER_REGISTRY}) may have issues (HTTP $HTTP_CODE)"
            WARNINGS=$((WARNINGS + 1))
          fi
          
          # Poetry install URL
          HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" --connect-timeout 10 "https://install.python-poetry.org")
          if [[ "$HTTP_CODE" =~ ^2 ]]; then
            echo "  âœ… Poetry install URL is accessible"
          else
            echo "  âš ï¸ Poetry install URL may not be accessible (HTTP $HTTP_CODE) - fallback to pip available"
            WARNINGS=$((WARNINGS + 1))
          fi
          
          # Docker install script
          HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" --connect-timeout 10 "https://get.docker.com")
          if [[ "$HTTP_CODE" =~ ^2 ]]; then
            echo "  âœ… Docker install script (get.docker.com) is accessible"
          else
            echo "  âš ï¸ Docker install script may not be accessible (HTTP $HTTP_CODE)"
            WARNINGS=$((WARNINGS + 1))
          fi
          
          # ===== Check required files =====
          echo ""
          echo "ðŸ“ [4/5] Checking required files..."
          echo "-------------------------------------------"
          
          # Check Docker Compose file
          if [ -f "${{ env.DOCKER_COMPOSE_FILE }}" ]; then
            echo "  âœ… Docker Compose file exists: ${{ env.DOCKER_COMPOSE_FILE }}"
          else
            echo "  âŒ Docker Compose file NOT found: ${{ env.DOCKER_COMPOSE_FILE }}"
            ERRORS=$((ERRORS + 1))
          fi
          
          # Check notebook file
          NOTEBOOK_PATH="${{ env.NOTEBOOK_RELATIVED_DIR }}/${{ matrix.NOTEBOOK_FILENAME }}"
          if [ -f "$NOTEBOOK_PATH" ]; then
            echo "  âœ… Notebook file exists: $NOTEBOOK_PATH"
          else
            echo "  âŒ Notebook file NOT found: $NOTEBOOK_PATH"
            ERRORS=$((ERRORS + 1))
          fi
          
          # Check requirements.txt
          if [ -f "requirements.txt" ]; then
            echo "  âœ… requirements.txt exists"
          else
            echo "  âš ï¸ requirements.txt NOT found (may cause issues in container)"
            WARNINGS=$((WARNINGS + 1))
          fi
          
          # ===== Check custom actions =====
          echo ""
          echo "ðŸŽ¬ [5/5] Checking custom actions..."
          echo "-------------------------------------------"
          
          if [ -d ".github/actions/setup-env" ]; then
            echo "  âœ… Custom action .github/actions/setup-env exists"
          else
            echo "  âŒ Custom action .github/actions/setup-env NOT found"
            ERRORS=$((ERRORS + 1))
          fi
          
          if [ -d ".github/actions/check-sysinfo" ]; then
            echo "  âœ… Custom action .github/actions/check-sysinfo exists"
          else
            echo "  âŒ Custom action .github/actions/check-sysinfo NOT found"
            ERRORS=$((ERRORS + 1))
          fi
          
          # ===== Final result =====
          echo ""
          echo "=========================================="
          echo "ðŸ“Š PRE-CHECK SUMMARY"
          echo "-------------------------------------------"
          echo "  Errors:   $ERRORS"
          echo "  Warnings: $WARNINGS"
          echo "=========================================="
          
          if [ $ERRORS -gt 0 ]; then
            echo ""
            echo "âŒ PRE-CHECK FAILED: $ERRORS error(s) detected"
            echo "Please fix the above errors before proceeding."
            echo "=========================================="
            exit 1
          else
            echo ""
            echo "âœ… PRE-CHECK PASSED: All critical prerequisites validated"
            if [ $WARNINGS -gt 0 ]; then
              echo "âš ï¸ Note: $WARNINGS warning(s) detected - please review above"
            fi
            echo "=========================================="
          fi

      - name: Checkout Test repository
        uses: actions/checkout@v4
        with:
          repository: "NVIDIA-AI-Blueprints/blueprint-github-test"
          token: ${{ secrets.BLUEPRINT_GITHUB_TEST_TOKEN_ON_GH }}
          path: blueprint-github-test
          fetch-depth: 1

      - uses: ./.github/actions/setup-env
        with:
          python-version: "3.12"
          check-disk: "false"

      - name: Setup docker env
        env:
          NGC_API_Key: ${{ secrets.NGC_API_KEY }}
        run: |
          # NOTE: Docker and tools already installed in pre-install step
          # This step now only verifies and displays system info
          
          echo "===================== System Info ====================="
          cat /etc/os-release
          echo ""
          echo "===================== Docker Info ====================="
          docker version
          docker compose version
          echo ""
          echo "===================== GPU Info ====================="
          nvidia-smi || echo "nvidia-smi not available (expected on non-GPU runners)"

      - uses: ./.github/actions/check-sysinfo

      - name: Ensure cache directory exists
        run: |
          HOST_CACHE_DIR="${{ env.HOST_CACHE_DIR }}"

          mkdir -p "$HOST_CACHE_DIR"
          sudo chown -R $(id -u):$(id -g) "$HOST_CACHE_DIR"
          ls -ld "$HOST_CACHE_DIR"

      - name: Cache Docker image
        id: cache-docker-image
        if: always()
        uses: actions/cache@v3
        with:
          path: ${{ env.HOST_CACHE_DIR }}
          key: "${{ steps.set_global_vars.outputs.cache_key }}"
          restore-keys: |
            ${{ runner.os }}-docker-

      - name: Debug info
        run: |
          echo "Primary Key: ${{ steps.cache-docker-image.outputs.cache-primary-key || 'N/A' }}"
          echo "Cache Hit: ${{ steps.cache-docker-image.outputs.cache-hit || 'false' }}"
          echo "Cache Path: ${{ env.HOST_CACHE_DIR }}"
          ls -la "${{ env.HOST_CACHE_DIR }}" || echo "Directory not found"

      - name: Load cached docker image
        if: steps.cache-docker-image.outputs.cache-hit == 'true'
        run: |
          HOST_CACHE_DIR="${{ env.HOST_CACHE_DIR }}"
          DOCKER_IMG_CACHE_NAME="${{ steps.set_global_vars.outputs.docker_img_cache_name }}"
          echo "Cached file with path: $HOST_CACHE_DIR/$DOCKER_IMG_CACHE_NAME"

          if [ -f "${HOST_CACHE_DIR}/${DOCKER_IMG_CACHE_NAME}.tar" ]; then
            echo "[Info]: the docker image cache exists, just load it from cache"
            docker load -i "${HOST_CACHE_DIR}/${DOCKER_IMG_CACHE_NAME}".tar
            echo "[Info]: the docker image cache exists, just load it from cache --- Done"
          fi

      - name: Pull docker image
        run: |
          start_time=$(date +%s)

          DOCKER_IMG_NAME="${{ env.DOCKER_IMG_NAME }}"
          DOCKER_IMG_TAG="${{ env.DOCKER_IMG_TAG }}"
          HOST_CACHE_DIR="${{ env.HOST_CACHE_DIR }}"
          DOCKER_IMG_CACHE_NAME="${{ steps.set_global_vars.outputs.docker_img_cache_name }}"

          echo "Docker image will use: $DOCKER_IMG_NAME:$DOCKER_IMG_TAG"

          if docker image inspect "$DOCKER_IMG_NAME:$DOCKER_IMG_TAG" >/dev/null 2>&1; then
            echo "[Info] Image already exists, skipping pull."
            exit 0
          fi

          echo "[Info]: the docker image cache does NOT exists, just pull it from remote"
          if ! docker pull "$DOCKER_IMG_NAME:$DOCKER_IMG_TAG"; then
            echo "Pull failed, retrying..."
            sleep 10
            docker pull --quiet "$DOCKER_IMG_NAME:$DOCKER_IMG_TAG" || {
              echo "Error: Docker pull failed after retry"
              exit 1
            }
          fi
          echo "[Info]: the docker image cache does NOT exists, just pull it from remote --- Done"


          mkdir -p ${HOST_CACHE_DIR}
          echo "[Info]: the docker image pull done, save it to cache path now"
          docker save $DOCKER_IMG_NAME:$DOCKER_IMG_TAG > ${HOST_CACHE_DIR}/${DOCKER_IMG_CACHE_NAME}.tar
          docker save -o ${HOST_CACHE_DIR}/${DOCKER_IMG_CACHE_NAME}.tar $DOCKER_IMG_NAME:$DOCKER_IMG_TAG
          ls -alh ${HOST_CACHE_DIR}
          echo "[Info]: the docker image pull done, save it to cache path now --- Done"

          end_time=$(date +%s)
          echo "Execution time: $((end_time - start_time)) seconds"
        timeout-minutes: 20

      - name: Verify cache
        run: |
          HOST_CACHE_DIR="${{ env.HOST_CACHE_DIR }}"
          du -sh ${HOST_CACHE_DIR}/* 2>/dev/null || echo "No cache files found"

      - name: Start container
        run: |
          start_time=$(date +%s)
          docker images -a
          echo "Docker compose will use file: $DOCKER_COMPOSE_FILE"


          DOCKER_IMG_TAG="${{ env.DOCKER_IMG_TAG }}"
          export DOCKER_IMG_TAG=$DOCKER_IMG_TAG
          export NOTEBOOKS_HOST_PATH="${{ github.workspace }}"

          echo "DOCKER_IMG_TAG: $DOCKER_IMG_TAG"
          echo "NOTEBOOKS_HOST_PATH: $NOTEBOOKS_HOST_PATH"

          pwd
          ls -al ./

          set -e
          export COMPOSE_HTTP_TIMEOUT=300
          
          # ============================================================================
          # CRITICAL: Create docker-compose override to inject UCX/RAFT env vars
          # Docker Compose doesn't inherit shell exports - must use override file
          # ============================================================================
          OVERRIDE_FILE="docker-compose.override.yml"
          echo "version: '3.8'" > "$OVERRIDE_FILE"
          echo "services:" >> "$OVERRIDE_FILE"
          echo "  backend:" >> "$OVERRIDE_FILE"
          echo "    environment:" >> "$OVERRIDE_FILE"
          echo "      UCX_TLS: tcp" >> "$OVERRIDE_FILE"
          echo "      UCX_MEMTYPE_CACHE: n" >> "$OVERRIDE_FILE"
          echo "      UCX_NET_DEVICES: all" >> "$OVERRIDE_FILE"
          echo "      UCX_LOG_LEVEL: error" >> "$OVERRIDE_FILE"
          echo "      UCX_WARN_UNUSED_ENV_VARS: n" >> "$OVERRIDE_FILE"
          echo "      RAFT_USE_UCX: '0'" >> "$OVERRIDE_FILE"
          echo "      RAFT_ENABLE_UCX: '0'" >> "$OVERRIDE_FILE"
          echo "      RAFT_COMMS_UCX_ENABLED: '0'" >> "$OVERRIDE_FILE"
          echo "      DASK_SCHEDULER: synchronous" >> "$OVERRIDE_FILE"
          echo "      DASK_UCX__TCP: 'true'" >> "$OVERRIDE_FILE"
          echo "      DASK_UCX__INFINIBAND: 'false'" >> "$OVERRIDE_FILE"
          echo "      DASK_UCX__NVLINK: 'false'" >> "$OVERRIDE_FILE"
          echo "      DASK_UCX__CUDA_COPY: 'false'" >> "$OVERRIDE_FILE"
          echo "      DASK_DATAFRAME__QUERY_PLANNING: 'False'" >> "$OVERRIDE_FILE"
          echo "      CUGRAPH_USE_LEGACY_ALGO: '1'" >> "$OVERRIDE_FILE"
          echo "      CUDA_LAUNCH_BLOCKING: '1'" >> "$OVERRIDE_FILE"
          echo "      RAPIDS_NO_INITIALIZE: '1'" >> "$OVERRIDE_FILE"
          echo "      OMP_NUM_THREADS: '1'" >> "$OVERRIDE_FILE"
          
          echo "=== Created docker-compose.override.yml with UCX/RAFT settings ==="
          cat "$OVERRIDE_FILE"
          echo "==================================================================="
          
          # Set COMPOSE_FILE for this step and persist to subsequent steps via GITHUB_ENV
          FULL_COMPOSE_FILE="${DOCKER_COMPOSE_FILE}:$(pwd)/${OVERRIDE_FILE}"
          export COMPOSE_FILE="$FULL_COMPOSE_FILE"
          echo "COMPOSE_FILE=$COMPOSE_FILE" >> $GITHUB_ENV
          echo "Using COMPOSE_FILE=$COMPOSE_FILE"
          
          docker compose --verbose up -d --wait --force-recreate

          end_time=$(date +%s)
          echo "Execution time: $((end_time - start_time)) seconds"

      - name: Check containers status
        run: |
          docker ps -a
          docker compose logs --no-color --tail=200 > docker-logs.txt
          head -c 500000 docker-logs.txt

      - name: Ensure container is ready for testing
        run: |
          docker compose exec -u root backend bash -c '
            apt-get update
            apt-get install -y libxml2-utils curl git
            apt-get clean
          '

          docker compose exec -T -u root backend bash <<'EOF'
          set -euxo pipefail

          whoami
          pwd
          ls -al ./
          ls -al ./notebooks/
          echo "Container working directory: $(pwd)"
          echo "Container contents:"
          ls -al ./
          echo "Notebooks directory contents:"
          ls -al ./notebooks/ 2>/dev/null || echo "notebooks directory not found"

          # Ensure we have git repository for blueprint-github-test
          if [ ! -d "blueprint-github-test" ]; then
            echo "Cloning blueprint-github-test repository..."
            git clone https://gitlab-master.nvidia.com/cloud-service-qa/Blueprint/blueprint-github-test.git || echo "Clone failed, will create directory"
            if [ ! -d "blueprint-github-test" ]; then
              mkdir -p blueprint-github-test
            fi
          fi

          python -m pip install --upgrade pip --user || true

          echo '=== Installing requirements ==='
          MAX_RETRIES=3
          RETRY_DELAY=5
          REQUIREMENTS_FILE="requirements.txt"

          for ((i=1; i<=MAX_RETRIES; i++)); do
              echo "Attempt #$i to install dependencies from: $REQUIREMENTS_FILE..."

              # Execute pip install and check exit status directly
              if pip install --user --no-cache-dir -r "$REQUIREMENTS_FILE"; then
                  echo "Dependencies installed successfully!"
                  break
              else
                  echo "Dependency installation failed (exit code: $?)."
                  if [ "$i" -lt "$MAX_RETRIES" ]; then
                      echo "Retrying in $RETRY_DELAY seconds..."
                      sleep "$RETRY_DELAY"
                  else
                      echo "Maximum retry attempts ($MAX_RETRIES) reached. Proceeding with subsequent commands."
                  fi
              fi
          done

          pip install ipykernel nbclient nbformat jupyter nbconvert pytest-html pytest-cov papermill
          python3 -m ipykernel install --user --name python3 --display-name "Python 3"

          echo '=== Verifying installations ==='
          pip list | grep -i papermill || { echo "ERROR: papermill not found in pip list"; exit 1; }
          
          echo '=== Verifying papermill command ==='
          python -m papermill --version || { echo "ERROR: papermill command failed"; exit 1; }
          
          echo '=== Verifying kernel ==='
          jupyter kernelspec list
          EOF

      - name: Check GPU status before notebook execution
        run: |
          echo "=============== GPU Status Before Notebook Execution ==============="
          docker compose exec -T -u root backend bash -c '
            echo "=== nvidia-smi ==="
            nvidia-smi
            
            echo ""
            echo "=== GPU Memory Summary ==="
            nvidia-smi --query-gpu=index,name,memory.total,memory.used,memory.free,utilization.gpu,utilization.memory --format=csv
            
            echo ""
            echo "=== GPU Processes ==="
            nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv 2>/dev/null || echo "No GPU processes running"
          '
          echo "===================================================================="

      - name: Run Jupyter Notebook
        id: run-jupyter-notebook
        timeout-minutes: 30
        run: |
          # NOTE: matrix expression won't be parsed in EOF
          NOTEBOOK_RELATIVED_DIR="${{ env.NOTEBOOK_RELATIVED_DIR }}"
          NOTEBOOK_FILENAME="${{ steps.set_global_vars.outputs.notebook_filename }}"
          NOTEBOOK_BASENAME="${{ steps.set_global_vars.outputs.notebook_basename }}"
          OUTPUT_NOTEBOOK="${{ env.OUTPUT_NOTEBOOK }}"
          OUTPUT_NOTEBOOK_HTML="${{ env.OUTPUT_NOTEBOOK_HTML }}"
          DOCKER_WRITEABLE_DIR="${{ env.DOCKER_WRITEABLE_DIR }}"

          echo "=============== Executing notebook with nbclient ====================="

          docker compose exec -u root backend bash <<EOF
          set -euxo pipefail
          export PYTHONUNBUFFERED=1
          
          # ============================================================================
          # Environment variables are set at container level via docker-compose.override.yml
          # These additional exports are belt-and-suspenders for the exec shell
          # ============================================================================
          export LD_PRELOAD=""
          export NUMBA_CUDA_USE_NVIDIA_BINDING=0
          
          echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
          echo "â•‘           DEBUG: Verify Container Environment Variables          â•‘"
          echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          
          echo ""
          echo "=== [1/6] UCX/RAFT/Dask Environment (set via docker-compose.override.yml) ==="
          echo "--- RAFT/UCX ---"
          echo "RAFT_USE_UCX=\${RAFT_USE_UCX:-NOT SET}"
          echo "RAFT_ENABLE_UCX=\${RAFT_ENABLE_UCX:-NOT SET}"
          echo "CUGRAPH_USE_LEGACY_ALGO=\${CUGRAPH_USE_LEGACY_ALGO:-NOT SET}"
          echo "--- Dask ---"
          echo "DASK_SCHEDULER=\${DASK_SCHEDULER:-NOT SET}"
          echo "--- UCX ---"
          echo "UCX_TLS=\${UCX_TLS:-NOT SET}"
          echo "--- CUDA ---"
          echo "CUDA_LAUNCH_BLOCKING=\${CUDA_LAUNCH_BLOCKING:-NOT SET}"
          echo "OMP_NUM_THREADS=\${OMP_NUM_THREADS:-NOT SET}"
          echo "CUDA_VISIBLE_DEVICES=\${CUDA_VISIBLE_DEVICES:-not set}"
          
          echo ""
          echo "=== [2/6] Python Package Versions ==="
          pip list 2>/dev/null | grep -iE "cudf|cupy|numba|rapids|cugraph|rmm" || echo "Package query failed"
          
          echo ""
          echo "=== [3/6] CUDA Device Detection ==="
          python3 -c "import os; print('CUDA_VISIBLE_DEVICES:', os.environ.get('CUDA_VISIBLE_DEVICES', 'not set'))"
          python3 -c "import cupy as cp; print('CuPy GPU count:', cp.cuda.runtime.getDeviceCount())" 2>/dev/null || echo "CuPy GPU detection failed"
          python3 -c "from numba import cuda; print('Numba GPU count:', len(cuda.gpus)); [print('  GPU', i, ':', g.name) for i,g in enumerate(cuda.gpus)]" 2>/dev/null || echo "Numba GPU detection failed"
          
          echo ""
          echo "=== [4/6] nvidia-smi GPU Status ==="
          nvidia-smi --query-gpu=index,name,memory.total,memory.used,memory.free,utilization.gpu,temperature.gpu --format=csv
          
          echo ""
          echo "=== [5/6] GPU Process Check ==="
          nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv 2>/dev/null || echo "No GPU processes running"
          
          echo ""
          echo "=== [6/6] UCX Info ==="
          ucx_info -v 2>/dev/null | head -5 || echo "ucx_info not available"
          
          echo ""
          echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
          echo "â•‘           Starting Notebook Execution                            â•‘"
          echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo ""

          python -m papermill "${NOTEBOOK_RELATIVED_DIR}/${NOTEBOOK_FILENAME}" "${DOCKER_WRITEABLE_DIR}/${OUTPUT_NOTEBOOK}" \
            --log-output \
            --log-level DEBUG \
            --progress-bar \
            --report-mode \
            --kernel python3 2>&1 | tee "${DOCKER_WRITEABLE_DIR}/papermill.log" || PAPERMILL_EXIT_CODE=\$?

          echo "=============== GPU Status AFTER Notebook Execution ==============="
          nvidia-smi --query-gpu=index,name,memory.total,memory.used,memory.free,utilization.gpu --format=csv 2>/dev/null || echo "WARNING: nvidia-smi failed - GPU may be in bad state"
          nvidia-smi 2>/dev/null || echo "WARNING: Full nvidia-smi failed"
          echo "===================================================================="

          # Convert executed notebook to HTML
          echo "=============== Converting Notebook to HTML ==============="
          if [ -f "${DOCKER_WRITEABLE_DIR}/${OUTPUT_NOTEBOOK}" ]; then
              echo "Converting ${DOCKER_WRITEABLE_DIR}/${OUTPUT_NOTEBOOK} to HTML..."
              jupyter nbconvert --to html "${DOCKER_WRITEABLE_DIR}/${OUTPUT_NOTEBOOK}" --output "${OUTPUT_NOTEBOOK_HTML}" --output-dir "${DOCKER_WRITEABLE_DIR}" 2>&1 || {
                  echo "ERROR: nbconvert failed, trying alternative method..."
                  python -m nbconvert --to html "${DOCKER_WRITEABLE_DIR}/${OUTPUT_NOTEBOOK}" --output "${OUTPUT_NOTEBOOK_HTML}" --output-dir "${DOCKER_WRITEABLE_DIR}" 2>&1 || {
                      echo "ERROR: Both nbconvert methods failed!"
                      ls -la "${DOCKER_WRITEABLE_DIR}/"
                  }
              }
          else
              echo "ERROR: Executed notebook not found at ${DOCKER_WRITEABLE_DIR}/${OUTPUT_NOTEBOOK}"
              ls -la "${DOCKER_WRITEABLE_DIR}/"
          fi
          echo "============================================================"

          if [ -s "${DOCKER_WRITEABLE_DIR}/${OUTPUT_NOTEBOOK_HTML}" ]; then
              echo "${DOCKER_WRITEABLE_DIR}/${OUTPUT_NOTEBOOK_HTML} exists and has contents"
          else
              echo "${DOCKER_WRITEABLE_DIR}/${OUTPUT_NOTEBOOK_HTML} does NOT exist or has NO contents"
              echo "Available files in ${DOCKER_WRITEABLE_DIR}:"
              ls -la "${DOCKER_WRITEABLE_DIR}/"
              exit 1
          fi

          echo '=== Papermill logs ==='
          cat "$DOCKER_WRITEABLE_DIR/papermill.log"
          
          # Exit with papermill's exit code if it failed
          if [ -n "\${PAPERMILL_EXIT_CODE:-}" ]; then
            echo "Papermill failed with exit code: \$PAPERMILL_EXIT_CODE"
            exit \$PAPERMILL_EXIT_CODE
          fi
          EOF

          echo "=============== Executing notebook --- Done ====================="

      - name: Set notebook execution result
        id: generate-notebook-html
        if: steps.run-jupyter-notebook.outcome == 'success'
        run: |
          echo "Notebook execution and HTML conversion completed successfully"
          OUTPUT_NOTEBOOK_HTML="${{ env.OUTPUT_NOTEBOOK_HTML }}"
          DOCKER_WRITEABLE_DIR="${{ env.DOCKER_WRITEABLE_DIR }}"

          docker compose exec -T -u root backend bash -c "
            echo '===============Check HTML file====================='
            if [ -s \"${DOCKER_WRITEABLE_DIR}/${OUTPUT_NOTEBOOK_HTML}\" ]; then
                echo \"${DOCKER_WRITEABLE_DIR}/${OUTPUT_NOTEBOOK_HTML} exists and has contents\"
            else
                echo \"${DOCKER_WRITEABLE_DIR}/${OUTPUT_NOTEBOOK_HTML} does NOT exist or has NO contents\"
            fi
            echo '===============Check file====================='
          "

      - name: Install Poetry/Dependencies and execute pytest
        id: run-pytest
        run: |
          SCRIPT_FILE=$(mktemp)
          cat > $SCRIPT_FILE <<'EOS'
          set -euxo pipefail
          cd /notebooks
          
          # Ensure blueprint-github-test exists and contains the test files
          if [ ! -d "blueprint-github-test" ]; then
            echo "blueprint-github-test directory not found, creating it..."
            mkdir -p blueprint-github-test
          fi
          
          cd blueprint-github-test

          OUTPUT_NOTEBOOK_HTML="$1"
          OUTPUT_PYTEST_COVERAGE_XML="$2"
          OUTPUT_PYTEST_RESULT_XML="$3"
          OUTPUT_PYTEST_REPORT_HTML="$4"
          DOCKER_WRITEABLE_DIR="$5"
          NOTEBOOK_BASENAME="$6"

          export PATH="/root/.local/bin:$PATH"
          python -m pip install --upgrade pip
          
          # Install poetry using pip if curl fails
          if command -v curl >/dev/null 2>&1; then
            curl -sSL https://install.python-poetry.org | python3 -
          else
            echo "curl not available, installing poetry via pip"
            python -m pip install poetry
          fi

          poetry config virtualenvs.create true
          poetry install --no-interaction --no-root
          source "$(poetry env info --path)/bin/activate"

          pip install pytest-cov

          if [ -d "cloudia" ]; then
            pushd cloudia
            pip install -e .
            popd
          fi

          rm -rf input/*
          mkdir -p input
          
          # Copy HTML file from the correct location
          if [ -f "$DOCKER_WRITEABLE_DIR/$OUTPUT_NOTEBOOK_HTML" ]; then
            cp "$DOCKER_WRITEABLE_DIR/$OUTPUT_NOTEBOOK_HTML" "input/$NOTEBOOK_BASENAME.html"
            echo "Copied HTML file: $DOCKER_WRITEABLE_DIR/$OUTPUT_NOTEBOOK_HTML -> input/$NOTEBOOK_BASENAME.html"
          else
            echo "HTML file not found at: $DOCKER_WRITEABLE_DIR/$OUTPUT_NOTEBOOK_HTML"
            echo "Available files in $DOCKER_WRITEABLE_DIR:"
            ls -la "$DOCKER_WRITEABLE_DIR/" | grep -E "\.(html|ipynb)$" || echo "No HTML/notebook files found"
            exit 1
          fi

          echo "Current directory: $(pwd)"
          echo "Python path: $(which python)"
          echo "Poetry env: $(poetry env info)"

          poetry run pytest -m single_cell --override-ini addopts="" \
            --cov=./ \
            --cov-report=xml \
            --junitxml="$DOCKER_WRITEABLE_DIR/$OUTPUT_PYTEST_RESULT_XML" \
            --html="$DOCKER_WRITEABLE_DIR/$OUTPUT_PYTEST_REPORT_HTML" \
            --self-contained-html || echo "Pytest execution completed"

          if [ -f "coverage.xml" ]; then
            mv coverage.xml "$DOCKER_WRITEABLE_DIR/$OUTPUT_PYTEST_COVERAGE_XML"
          fi
          EOS

          docker compose exec -T -u root -w /notebooks backend bash -s -- \
            "${{ env.OUTPUT_NOTEBOOK_HTML }}" \
            "${{ env.OUTPUT_PYTEST_COVERAGE_XML }}" \
            "${{ env.OUTPUT_PYTEST_RESULT_XML }}" \
            "${{ env.OUTPUT_PYTEST_REPORT_HTML }}" \
            "${{ env.DOCKER_WRITEABLE_DIR }}" \
            "${{ steps.set_global_vars.outputs.notebook_basename }}" \
            < "$SCRIPT_FILE" || echo "Docker exec completed with status $?"
        shell: bash

      - name: Copy result files from container to runner and display test results
        id: prepare-result-file
        if: always() && steps.generate-notebook-html.outcome == 'success'
        run: |
          OUTPUT_NOTEBOOK_HTML="${{ env.OUTPUT_NOTEBOOK_HTML }}"
          OUTPUT_PYTEST_COVERAGE_XML="${{ env.OUTPUT_PYTEST_COVERAGE_XML }}"
          OUTPUT_PYTEST_RESULT_XML="${{ env.OUTPUT_PYTEST_RESULT_XML }}"
          OUTPUT_PYTEST_REPORT_HTML="${{ env.OUTPUT_PYTEST_REPORT_HTML }}"
          DOCKER_WRITEABLE_DIR="${{ env.DOCKER_WRITEABLE_DIR }}"
          ARTIFACT_DIR="${{ env.ARTIFACT_DIR }}"

          mkdir -p "$ARTIFACT_DIR" || exit 1
          
          # Copy all generated files from container
          docker compose cp backend:"$DOCKER_WRITEABLE_DIR/$OUTPUT_NOTEBOOK_HTML" "$ARTIFACT_DIR/" 2>/dev/null || echo "Note: $OUTPUT_NOTEBOOK_HTML not found"
          docker compose cp backend:"$DOCKER_WRITEABLE_DIR/$OUTPUT_PYTEST_COVERAGE_XML" "$ARTIFACT_DIR/" 2>/dev/null || echo "Note: $OUTPUT_PYTEST_COVERAGE_XML not found"
          docker compose cp backend:"$DOCKER_WRITEABLE_DIR/$OUTPUT_PYTEST_RESULT_XML" "$ARTIFACT_DIR/" 2>/dev/null || echo "Note: $OUTPUT_PYTEST_RESULT_XML not found"
          docker compose cp backend:"$DOCKER_WRITEABLE_DIR/$OUTPUT_PYTEST_REPORT_HTML" "$ARTIFACT_DIR/" 2>/dev/null || echo "Note: $OUTPUT_PYTEST_REPORT_HTML not found"

          echo "### ðŸ“Š Test Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Notebook:** ${{ matrix.NOTEBOOK_FILENAME }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Display pytest HTML report content if available
          if [ -f "$ARTIFACT_DIR/$OUTPUT_PYTEST_REPORT_HTML" ]; then
            echo "**ðŸ“‹ Pytest Report:**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "The detailed test report has been generated and uploaded to artifacts." >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "ðŸ“Š [Download Test Report]($GITHUB_SERVER_URL/$GITHUB_REPOSITORY/actions/runs/$GITHUB_RUN_ID)" >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ Pytest HTML report not generated" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**ðŸ“ Generated Files:**" >> $GITHUB_STEP_SUMMARY
          ls -la "$ARTIFACT_DIR" | while read line; do
            echo "- $line" >> $GITHUB_STEP_SUMMARY
          done

      - name: Upload all result files as artifacts
        if: steps.prepare-result-file.outcome == 'success'
        uses: actions/upload-artifact@v4
        with:
          name: ${{ steps.set_global_vars.outputs.artifact_name }}
          path: ${{ env.ARTIFACT_DIR }}/
          retention-days: 30
          if-no-files-found: warn

      - name: Stop containers
        if: ${{ always() }}
        run: docker compose down

      - name: Clean up
        if: always()
        run: |
          docker system prune -f

  summary:
    needs: run-notebook
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Query cache via API
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          for i in {1..3}; do
            echo "Attempt $i: Querying cache..."
            response=$(curl -s \
              -H "Authorization: Bearer $GITHUB_TOKEN" \
              -H "Accept: application/vnd.github.v3+json" \
              "https://api.github.com/repos/${{ github.repository }}/actions/caches?key=${{ runner.os }}-docker-")

            if echo "$response" | grep -q "${{ runner.os }}-docker-"; then
              echo "$response" | jq .
              exit 0
            else
              echo "Cache not found. Retrying in 10 seconds..."
              sleep 10
            fi
          done
          echo "Error: Cache not found after 3 attempts."
          exit 1

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: downloaded-results

      - name: Generate final summary
        run: |
          echo "### ðŸ“Š Notebook Execution Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Count executed notebooks by checking for artifacts
          executed_count=0
          if [ -d "downloaded-results" ]; then
            for result_dir in downloaded-results/results-*/; do
              if [ -d "$result_dir" ]; then
                executed_count=$((executed_count + 1))
                notebook_name=$(basename "$result_dir" | sed 's/^results-//')
                echo "âœ… **$notebook_name** - Execution completed" >> $GITHUB_STEP_SUMMARY
                
                # List available files for this notebook
                echo "   - Available files:" >> $GITHUB_STEP_SUMMARY
                ls -1 "$result_dir" | while read file; do
                  echo "     - $file" >> $GITHUB_STEP_SUMMARY
                done
                echo "" >> $GITHUB_STEP_SUMMARY
              fi
            done
          fi

          if [ $executed_count -eq 1 ]; then
            echo "**Summary:** $executed_count notebook executed successfully" >> $GITHUB_STEP_SUMMARY
          else
            echo "**Summary:** $executed_count notebooks executed successfully" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ“Š [Download All Artifacts]($GITHUB_SERVER_URL/$GITHUB_REPOSITORY/actions/runs/$GITHUB_RUN_ID)" >> $GITHUB_STEP_SUMMARY
